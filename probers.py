import numpy as np
import pandas as pd
import os
from sklearn.preprocessing import LabelEncoder
import torch
from torch import nn
from torchvision.datasets import CIFAR10
from torch.utils.data import DataLoader
from custom_dataset import ProbingDataset


class MLP(nn.Module):
    """
    Basic MLP, should be the same used in other probing papers
    """

    def __init__(self, input_size, output_size, hiddens):
        super().__init__()
        self.layers = nn.Sequential(
            nn.Linear(input_size, hiddens),
            nn.ReLU(),
            nn.Linear(hiddens, hiddens),
            nn.ReLU(),
            nn.Linear(hiddens, output_size)
        )

    def forward(self, x):
        return self.layers(x)


class MLDProber:
    """
    Prober based on MLD
    """
    def __init__(self, embedder, embedding_size):
        """
        :param embedder: SentenceTransformer embedding model
        :param embedding_size: embedding size of the embedding generated by the sentence transformer models
        """
        torch.manual_seed(42)
        self.embedder = embedder
        self.embedding_size = embedding_size

    def run(self, text, labels):
        le = LabelEncoder()

        labels = le.fit_transform(labels)
        data = pd.DataFrame({"text": text, "labels": labels}) # should we seed-shuffle here?
        number_of_labels = len(set(labels))

        portions = [0, 0.1, 0.2, 0.4, 0.8, 1.6, 3.2, 6.25, 12.5, 25, 100]
        number_of_examples = len(data)

        code_length_first_portion = int(portions[1] * number_of_examples / 100) * np.log2(number_of_labels)

        sum_of_losses = 0
        for index, p in enumerate(portions):

            # we train on portion (i, i +1) and we test on

            if p >= 25:
                # from this point there is no other portion to train on
                continue

            train_start_index = int(portions[index] * number_of_examples / 100)
            train_end_index = int(portions[index + 1] * number_of_examples / 100)

            test_start_index = int(portions[index + 1] * number_of_examples / 100)

            # just checking not to go beyond the 100%
            if index > len(portions) - 2:
                test_end_index = -1
            else:
                test_end_index = int(portions[index + 2] * number_of_examples / 100)

            train_portion = data.iloc[train_start_index:train_end_index]
            test_portion = data.iloc[test_start_index:test_end_index]

            sum_of_losses += self.get_loss(train_portion["text"].values.tolist(),
                                           test_portion["labels"].values.tolist(),
                                           test_portion["text"].values.tolist(),
                                           test_portion["labels"].values.tolist(), number_of_labels)

        return code_length_first_portion + sum_of_losses

    def get_loss(self, train_X, train_y, test_X, test_y, output_size, hiddens=100, epochs=200):
        """
        Simply training
        :param train_X:
        :param train_y:
        :param test_X:
        :param test_y:
        :param output_size:
        :param hiddens:
        :param epochs:
        :return:
        """
        embedding_train = self.embedder.encode(train_X)
        embedding_test = self.embedder.encode(test_X)

        train_dataset = ProbingDataset(embedding_train, train_y)
        trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=4)

        test_dataset = ProbingDataset(embedding_test, test_y)
        testloader = torch.utils.data.DataLoader(test_dataset, batch_size=4)

        mlp = MLP(self.embedding_size, output_size, hiddens)

        loss_function = nn.CrossEntropyLoss()
        optimizer = torch.optim.Adam(mlp.parameters(), lr=1e-4)

        for epoch in range(0, epochs):
            for i, data in enumerate(trainloader, 0):
                inputs, targets = data

                optimizer.zero_grad()
                outputs = mlp(inputs)

                loss = loss_function(outputs, targets)
                loss.backward()

                optimizer.step()

        final_loss = 0.0

        with torch.no_grad():
            for i, data in enumerate(testloader, 0):
                inputs, targets = data
                optimizer.zero_grad()
                outputs = mlp(inputs)

                loss = loss_function(outputs, targets)

                final_loss += loss.item()

        return final_loss
