{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinid/.local/lib/python3.8/site-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.9) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "comet_ml is installed but `COMET_API_KEY` is not set.\n"
     ]
    }
   ],
   "source": [
    "from probers import ClassicalProber\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://github.com/MilaNLProc/translation_bias/raw/master/data/en_us/en_us_TRAIN.xlsx\n",
    "#!wget https://github.com/MilaNLProc/translation_bias/raw/master/data/en_us/en_us_TEST.xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_train = \"en_us_TRAIN.xlsx\"\n",
    "english_test = \"en_us_TEST.xlsx\"\n",
    "\n",
    "\n",
    "english_train = pd.read_excel(english_train)\n",
    "english_train = english_train.dropna()\n",
    "english_train = english_train[english_train[\"text\"].apply(lambda x : len(x.split()) < 400)]\n",
    "\n",
    "\n",
    "english_test = pd.read_excel(english_test)\n",
    "english_test = english_test.dropna()\n",
    "english_test = english_test[english_test[\"text\"].apply(lambda x : len(x.split()) < 400)]\n",
    "\n",
    "english_train = pd.concat([english_test, english_train]).sample(frac=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = \"roberta-base-1B-1,roberta-base-1B-2,roberta-base-1B-3,roberta-base-100M-1,roberta-base-100M-2,roberta-base-100M-3,roberta-base-10M-1,roberta-base-10M-2,roberta-base-10M-3,roberta-med-small-1M-1,roberta-med-small-1M-2,roberta-med-small-1M-3\".split(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinid/.local/lib/python3.8/site-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.9) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "comet_ml is installed but `COMET_API_KEY` is not set.\n"
     ]
    }
   ],
   "source": [
    "from probers import Embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at nyu-mll/roberta-med-small-1M-3 were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "embe = Embedder(\"nyu-mll/roberta-med-small-1M-3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50ccb897619748fa889d45a78e8744aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "100%|██████████| 1345/1345 [11:18<00:00,  1.98it/s]\n"
     ]
    }
   ],
   "source": [
    "embe.create_embeddings(english_train[\"text\"].values.tolist(), english_train[\"age_cat\"].values.tolist(), [1, 2], \"embeddings/test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>age</th>\n",
       "      <th>age_cat</th>\n",
       "      <th>gender</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3312</th>\n",
       "      <td>3715</td>\n",
       "      <td>I am in the throes of getting a passport for m...</td>\n",
       "      <td>66</td>\n",
       "      <td>65+</td>\n",
       "      <td>F</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>133</td>\n",
       "      <td>I'm not only sleeping better but I wake up wit...</td>\n",
       "      <td>42</td>\n",
       "      <td>25-54</td>\n",
       "      <td>M</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1284</th>\n",
       "      <td>1672</td>\n",
       "      <td>I have used Next for many years for online sho...</td>\n",
       "      <td>43</td>\n",
       "      <td>25-54</td>\n",
       "      <td>F</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>755</th>\n",
       "      <td>1131</td>\n",
       "      <td>They have tons of supplements I didnt even kno...</td>\n",
       "      <td>23</td>\n",
       "      <td>15-24</td>\n",
       "      <td>M</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4790</th>\n",
       "      <td>5193</td>\n",
       "      <td>I have used this website several times, and ev...</td>\n",
       "      <td>73</td>\n",
       "      <td>65+</td>\n",
       "      <td>F</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2945</th>\n",
       "      <td>3348</td>\n",
       "      <td>This company by far is superior in prices alon...</td>\n",
       "      <td>34</td>\n",
       "      <td>25-54</td>\n",
       "      <td>M</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3053</th>\n",
       "      <td>3456</td>\n",
       "      <td>I just started using Amazon within the last 9 ...</td>\n",
       "      <td>28</td>\n",
       "      <td>25-54</td>\n",
       "      <td>M</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>838</td>\n",
       "      <td>Fantastic selection of items at affordable pri...</td>\n",
       "      <td>45</td>\n",
       "      <td>25-54</td>\n",
       "      <td>M</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2748</th>\n",
       "      <td>3151</td>\n",
       "      <td>Great pricing, a vast assortment of products a...</td>\n",
       "      <td>27</td>\n",
       "      <td>25-54</td>\n",
       "      <td>M</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2894</th>\n",
       "      <td>3297</td>\n",
       "      <td>Quick delivery. Arrived as ordered. Can't comp...</td>\n",
       "      <td>38</td>\n",
       "      <td>25-54</td>\n",
       "      <td>M</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5377 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                               text  age  \\\n",
       "3312        3715  I am in the throes of getting a passport for m...   66   \n",
       "133          133  I'm not only sleeping better but I wake up wit...   42   \n",
       "1284        1672  I have used Next for many years for online sho...   43   \n",
       "755         1131  They have tons of supplements I didnt even kno...   23   \n",
       "4790        5193  I have used this website several times, and ev...   73   \n",
       "...          ...                                                ...  ...   \n",
       "2945        3348  This company by far is superior in prices alon...   34   \n",
       "3053        3456  I just started using Amazon within the last 9 ...   28   \n",
       "465          838  Fantastic selection of items at affordable pri...   45   \n",
       "2748        3151  Great pricing, a vast assortment of products a...   27   \n",
       "2894        3297  Quick delivery. Arrived as ordered. Can't comp...   38   \n",
       "\n",
       "     age_cat gender  label  \n",
       "3312     65+      F  train  \n",
       "133    25-54      M   test  \n",
       "1284   25-54      F  train  \n",
       "755    15-24      M  train  \n",
       "4790     65+      F  train  \n",
       "...      ...    ...    ...  \n",
       "2945   25-54      M  train  \n",
       "3053   25-54      M  train  \n",
       "465    25-54      M  train  \n",
       "2748   25-54      M  train  \n",
       "2894   25-54      M  train  \n",
       "\n",
       "[5377 rows x 6 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embe.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"embeddings/trust_test.pkl\", \"rb\") as flino:\n",
    "    data = pickle.load(flino)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 0.2909695583542671, 2: 0.27425049581022043}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bibi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (inf --> 178.704727).  Saving model ...\n",
      "Validation loss decreased (178.704727 --> 173.677795).  Saving model ...\n",
      "Validation loss decreased (173.677795 --> 172.133072).  Saving model ...\n",
      "Validation loss decreased (172.133072 --> 171.111877).  Saving model ...\n",
      "Validation loss decreased (171.111877 --> 170.350143).  Saving model ...\n",
      "Validation loss decreased (170.350143 --> 169.440842).  Saving model ...\n",
      "Validation loss decreased (169.440842 --> 168.611633).  Saving model ...\n",
      "Validation loss decreased (168.611633 --> 167.835861).  Saving model ...\n",
      "Validation loss decreased (167.835861 --> 167.144012).  Saving model ...\n",
      "Validation loss decreased (167.144012 --> 166.269180).  Saving model ...\n",
      "Validation loss decreased (166.269180 --> 165.454056).  Saving model ...\n",
      "Validation loss decreased (165.454056 --> 164.710495).  Saving model ...\n",
      "Validation loss decreased (164.710495 --> 163.932236).  Saving model ...\n",
      "Validation loss decreased (163.932236 --> 163.204941).  Saving model ...\n",
      "Validation loss decreased (163.204941 --> 162.524918).  Saving model ...\n",
      "Validation loss decreased (162.524918 --> 161.864090).  Saving model ...\n",
      "Validation loss decreased (161.864090 --> 161.231415).  Saving model ...\n",
      "Validation loss decreased (161.231415 --> 160.661163).  Saving model ...\n",
      "Validation loss decreased (160.661163 --> 160.114456).  Saving model ...\n",
      "Validation loss decreased (160.114456 --> 159.602859).  Saving model ...\n",
      "Validation loss decreased (159.602859 --> 159.121078).  Saving model ...\n",
      "Validation loss decreased (159.121078 --> 158.682922).  Saving model ...\n",
      "Validation loss decreased (158.682922 --> 158.268845).  Saving model ...\n",
      "Validation loss decreased (158.268845 --> 157.875000).  Saving model ...\n",
      "Validation loss decreased (157.875000 --> 157.514786).  Saving model ...\n",
      "Validation loss decreased (157.514786 --> 157.166626).  Saving model ...\n",
      "Validation loss decreased (157.166626 --> 156.849625).  Saving model ...\n",
      "Validation loss decreased (156.849625 --> 156.543274).  Saving model ...\n",
      "Validation loss decreased (156.543274 --> 156.264801).  Saving model ...\n",
      "Validation loss decreased (156.264801 --> 156.010880).  Saving model ...\n",
      "Validation loss decreased (156.010880 --> 155.790619).  Saving model ...\n",
      "Validation loss decreased (155.790619 --> 155.596527).  Saving model ...\n",
      "Validation loss decreased (155.596527 --> 155.419739).  Saving model ...\n",
      "Validation loss decreased (155.419739 --> 155.247482).  Saving model ...\n",
      "Validation loss decreased (155.247482 --> 155.091522).  Saving model ...\n",
      "Validation loss decreased (155.091522 --> 154.967773).  Saving model ...\n",
      "Validation loss decreased (154.967773 --> 154.847610).  Saving model ...\n",
      "Validation loss decreased (154.847610 --> 154.732880).  Saving model ...\n",
      "Validation loss decreased (154.732880 --> 154.631119).  Saving model ...\n",
      "Validation loss decreased (154.631119 --> 154.530228).  Saving model ...\n",
      "Validation loss decreased (154.530228 --> 154.461655).  Saving model ...\n",
      "Validation loss decreased (154.461655 --> 154.412338).  Saving model ...\n",
      "Validation loss decreased (154.412338 --> 154.346359).  Saving model ...\n",
      "Validation loss decreased (154.346359 --> 154.311813).  Saving model ...\n",
      "Validation loss decreased (154.311813 --> 154.284836).  Saving model ...\n",
      "Validation loss decreased (154.284836 --> 154.242035).  Saving model ...\n",
      "Validation loss decreased (154.242035 --> 154.216339).  Saving model ...\n",
      "Validation loss decreased (154.216339 --> 154.203461).  Saving model ...\n",
      "Validation loss decreased (154.203461 --> 154.195236).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 1\n",
      "Early stopping\n",
      "Validation loss decreased (inf --> 177.910233).  Saving model ...\n",
      "Validation loss decreased (177.910233 --> 171.578140).  Saving model ...\n",
      "Validation loss decreased (171.578140 --> 170.102707).  Saving model ...\n",
      "Validation loss decreased (170.102707 --> 169.111923).  Saving model ...\n",
      "Validation loss decreased (169.111923 --> 168.191452).  Saving model ...\n",
      "Validation loss decreased (168.191452 --> 167.300903).  Saving model ...\n",
      "Validation loss decreased (167.300903 --> 166.413330).  Saving model ...\n",
      "Validation loss decreased (166.413330 --> 165.553818).  Saving model ...\n",
      "Validation loss decreased (165.553818 --> 164.695007).  Saving model ...\n",
      "Validation loss decreased (164.695007 --> 163.893585).  Saving model ...\n",
      "Validation loss decreased (163.893585 --> 163.171402).  Saving model ...\n",
      "Validation loss decreased (163.171402 --> 162.527390).  Saving model ...\n",
      "Validation loss decreased (162.527390 --> 161.963074).  Saving model ...\n",
      "Validation loss decreased (161.963074 --> 161.458450).  Saving model ...\n",
      "Validation loss decreased (161.458450 --> 161.012421).  Saving model ...\n",
      "Validation loss decreased (161.012421 --> 160.607285).  Saving model ...\n",
      "Validation loss decreased (160.607285 --> 160.265747).  Saving model ...\n",
      "Validation loss decreased (160.265747 --> 159.948700).  Saving model ...\n",
      "Validation loss decreased (159.948700 --> 159.663193).  Saving model ...\n",
      "Validation loss decreased (159.663193 --> 159.393158).  Saving model ...\n",
      "Validation loss decreased (159.393158 --> 159.170029).  Saving model ...\n",
      "Validation loss decreased (159.170029 --> 158.976730).  Saving model ...\n",
      "Validation loss decreased (158.976730 --> 158.787354).  Saving model ...\n",
      "Validation loss decreased (158.787354 --> 158.649063).  Saving model ...\n",
      "Validation loss decreased (158.649063 --> 158.492889).  Saving model ...\n",
      "Validation loss decreased (158.492889 --> 158.380859).  Saving model ...\n",
      "Validation loss decreased (158.380859 --> 158.271957).  Saving model ...\n",
      "Validation loss decreased (158.271957 --> 158.177383).  Saving model ...\n",
      "Validation loss decreased (158.177383 --> 158.091904).  Saving model ...\n",
      "Validation loss decreased (158.091904 --> 158.023788).  Saving model ...\n",
      "Validation loss decreased (158.023788 --> 157.968781).  Saving model ...\n",
      "Validation loss decreased (157.968781 --> 157.945938).  Saving model ...\n",
      "Validation loss decreased (157.945938 --> 157.916260).  Saving model ...\n",
      "Validation loss decreased (157.916260 --> 157.875229).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 1\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "classic = ClassicalProber(512)\n",
    "\n",
    "bibi = classic.run(\"embeddings/test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting datasets\n",
      "  Downloading datasets-2.1.0-py3-none-any.whl (325 kB)\n",
      "\u001b[K     |████████████████████████████████| 325 kB 5.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: huggingface-hub<1.0.0,>=0.1.0 in /home/vinid/.local/lib/python3.8/site-packages (from datasets) (0.2.1)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.17 in /home/vinid/.local/lib/python3.8/site-packages (from datasets) (1.19.3)\n",
      "Requirement already satisfied, skipping upgrade: fsspec[http]>=2021.05.0 in /home/vinid/.local/lib/python3.8/site-packages (from datasets) (2021.5.0)\n",
      "Requirement already satisfied, skipping upgrade: dill in /home/vinid/.local/lib/python3.8/site-packages (from datasets) (0.3.3)\n",
      "Collecting responses<0.19\n",
      "  Using cached responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied, skipping upgrade: pandas in /home/vinid/.local/lib/python3.8/site-packages (from datasets) (1.2.0)\n",
      "Requirement already satisfied, skipping upgrade: multiprocess in /home/vinid/.local/lib/python3.8/site-packages (from datasets) (0.70.12.2)\n",
      "Requirement already satisfied, skipping upgrade: aiohttp in /home/vinid/.local/lib/python3.8/site-packages (from datasets) (3.6.2)\n",
      "Requirement already satisfied, skipping upgrade: requests>=2.19.0 in /home/vinid/.local/lib/python3.8/site-packages (from datasets) (2.23.0)\n",
      "Requirement already satisfied, skipping upgrade: packaging in /home/vinid/.local/lib/python3.8/site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied, skipping upgrade: tqdm>=4.62.1 in /home/vinid/.local/lib/python3.8/site-packages (from datasets) (4.62.3)\n",
      "Collecting pyarrow>=5.0.0\n",
      "  Downloading pyarrow-8.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 29.4 MB 19.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: xxhash in /home/vinid/.local/lib/python3.8/site-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied, skipping upgrade: typing-extensions>=3.7.4.3 in /home/vinid/.local/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.7.4.3)\n",
      "Requirement already satisfied, skipping upgrade: pyyaml in /usr/lib/python3/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (5.3.1)\n",
      "Requirement already satisfied, skipping upgrade: filelock in /home/vinid/.local/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.0.12)\n",
      "Collecting urllib3>=1.25.10\n",
      "  Using cached urllib3-1.26.9-py2.py3-none-any.whl (138 kB)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.7.3 in /home/vinid/.local/lib/python3.8/site-packages (from pandas->datasets) (2.8.1)\n",
      "Requirement already satisfied, skipping upgrade: pytz>=2017.3 in /usr/lib/python3/dist-packages (from pandas->datasets) (2019.3)\n",
      "Requirement already satisfied, skipping upgrade: attrs>=17.3.0 in /home/vinid/.local/lib/python3.8/site-packages (from aiohttp->datasets) (19.1.0)\n",
      "Requirement already satisfied, skipping upgrade: async-timeout<4.0,>=3.0 in /home/vinid/.local/lib/python3.8/site-packages (from aiohttp->datasets) (3.0.1)\n",
      "Requirement already satisfied, skipping upgrade: yarl<2.0,>=1.0 in /home/vinid/.local/lib/python3.8/site-packages (from aiohttp->datasets) (1.5.1)\n",
      "Requirement already satisfied, skipping upgrade: multidict<5.0,>=4.5 in /home/vinid/.local/lib/python3.8/site-packages (from aiohttp->datasets) (4.7.6)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4.0,>=2.0 in /usr/lib/python3/dist-packages (from aiohttp->datasets) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /home/vinid/.local/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (2020.4.5.1)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /home/vinid/.local/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (2.9)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing!=3.0.5,>=2.0.2 in /home/vinid/.local/lib/python3.8/site-packages (from packaging->datasets) (2.4.7)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5 in /home/vinid/.local/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
      "Installing collected packages: urllib3, responses, pyarrow, datasets\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.25.9\n",
      "    Uninstalling urllib3-1.25.9:\n",
      "      Successfully uninstalled urllib3-1.25.9\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 3.0.0\n",
      "    Uninstalling pyarrow-3.0.0:\n",
      "      Successfully uninstalled pyarrow-3.0.0\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 1.8.0\n",
      "    Uninstalling datasets-1.8.0:\n",
      "      Successfully uninstalled datasets-1.8.0\n",
      "\u001b[31mERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\n",
      "\n",
      "We recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\n",
      "\n",
      "snowflake-connector-python 2.3.3 requires boto3<1.15,>=1.4.4, but you'll have boto3 1.15.8 which is incompatible.\n",
      "snowflake-connector-python 2.3.3 requires urllib3<1.26.0,>=1.20, but you'll have urllib3 1.26.9 which is incompatible.\n",
      "requests 2.23.0 requires urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you'll have urllib3 1.26.9 which is incompatible.\n",
      "label-studio 0.7.4.post0 requires appdirs==1.4.3, but you'll have appdirs 1.4.4 which is incompatible.\n",
      "label-studio 0.7.4.post0 requires certifi==2019.11.28, but you'll have certifi 2020.4.5.1 which is incompatible.\n",
      "label-studio 0.7.4.post0 requires Click==7.0, but you'll have click 7.1.2 which is incompatible.\n",
      "label-studio 0.7.4.post0 requires Flask==1.1.1, but you'll have flask 1.1.2 which is incompatible.\n",
      "label-studio 0.7.4.post0 requires idna==2.8, but you'll have idna 2.9 which is incompatible.\n",
      "label-studio 0.7.4.post0 requires numpy==1.17.4, but you'll have numpy 1.19.3 which is incompatible.\n",
      "label-studio 0.7.4.post0 requires requests==2.22.0, but you'll have requests 2.23.0 which is incompatible.\n",
      "label-studio 0.7.4.post0 requires six==1.12.0, but you'll have six 1.15.0 which is incompatible.\n",
      "label-studio 0.7.4.post0 requires srsly==0.2.0, but you'll have srsly 2.4.0 which is incompatible.\n",
      "label-studio 0.7.4.post0 requires tqdm==4.40.2, but you'll have tqdm 4.62.3 which is incompatible.\n",
      "label-studio 0.7.4.post0 requires urllib3==1.25.7, but you'll have urllib3 1.26.9 which is incompatible.\n",
      "label-studio 0.7.4.post0 requires wasabi==0.4.2, but you'll have wasabi 0.8.2 which is incompatible.\n",
      "label-studio-converter 0.0.18 requires Pillow==6.2.1, but you'll have pillow 8.3.2 which is incompatible.\n",
      "label-studio-converter 0.0.18 requires requests==2.22.0, but you'll have requests 2.23.0 which is incompatible.\n",
      "google-api-core 1.22.2 requires google-auth<2.0dev,>=1.21.1, but you'll have google-auth 1.20.1 which is incompatible.\n",
      "botometer 1.6.1 requires tweepy<4,>=3.5.0, but you'll have tweepy 4.4.0 which is incompatible.\n",
      "botocore 1.18.18 requires urllib3<1.26,>=1.20; python_version != \"3.4\", but you'll have urllib3 1.26.9 which is incompatible.\n",
      "allennlp 1.1.0 requires spacy<2.4,>=2.1.0, but you'll have spacy 3.0.5 which is incompatible.\n",
      "allennlp 1.1.0 requires torch<1.7.0,>=1.6.0, but you'll have torch 1.9.0 which is incompatible.\n",
      "allennlp 1.1.0 requires transformers<3.1,>=3.0, but you'll have transformers 4.12.5 which is incompatible.\u001b[0m\n",
      "Successfully installed datasets-2.1.0 pyarrow-8.0.0 responses-0.18.0 urllib3-1.26.9\n",
      "\u001b[33mWARNING: You are using pip version 20.2.2; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -U datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "results = defaultdict(list)\n",
    "\n",
    "for m in models:\n",
    "    for r in [0, 1, 2]:\n",
    "        st = SentenceTransformer(r)\n",
    "\n",
    "        mldprober = ClassicalProber(st, 512)\n",
    "\n",
    "        results[m].append(mldprober.run(english_train[\"text\"].values.tolist(), english_train[\"gender\"].values.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca927491be704af687f17f3f1df37573",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/391 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6caffa43bfb043f1a3603212adc05312",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/9.08k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4a71304596c410db638455f34b77f13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "043a6d1158f94307bc53e9f67ee2c22a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/603k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76f04c4145a84f61b0c86f24d1548255",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "827a264d04bf4ed8bc542866be3afa85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/501M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fca5ff831a94672927d2ea5800b2a2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d742037ff8a44218ad82b92f8f40793",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /home/vinid/.cache/torch/sentence_transformers/roberta-base. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at /home/vinid/.cache/torch/sentence_transformers/roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (inf --> 70.219276).  Saving model ...\n",
      "Validation loss decreased (70.219276 --> 67.902344).  Saving model ...\n",
      "Validation loss decreased (67.902344 --> 66.250961).  Saving model ...\n",
      "Validation loss decreased (66.250961 --> 65.201897).  Saving model ...\n",
      "Validation loss decreased (65.201897 --> 64.648849).  Saving model ...\n",
      "Validation loss decreased (64.648849 --> 64.380066).  Saving model ...\n",
      "Validation loss decreased (64.380066 --> 64.235428).  Saving model ...\n",
      "Validation loss decreased (64.235428 --> 64.194435).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 5\n",
      "EarlyStopping counter: 2 out of 5\n",
      "EarlyStopping counter: 3 out of 5\n",
      "EarlyStopping counter: 4 out of 5\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "st = SentenceTransformer(\"roberta-base\")\n",
    "\n",
    "mldprober = ClassicalProber(st, 768)\n",
    "\n",
    "predicitions = mldprober.run(english_train[\"text\"].values.tolist(), english_train[\"gender\"].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.62      0.66       493\n",
      "           1       0.67      0.75      0.71       503\n",
      "\n",
      "    accuracy                           0.69       996\n",
      "   macro avg       0.69      0.69      0.69       996\n",
      "weighted avg       0.69      0.69      0.69       996\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(predicitions[1], predicitions[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.54      0.59       493\n",
      "           1       0.61      0.71      0.66       503\n",
      "\n",
      "    accuracy                           0.63       996\n",
      "   macro avg       0.63      0.62      0.62       996\n",
      "weighted avg       0.63      0.63      0.62       996\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predicitions[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicitions[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
